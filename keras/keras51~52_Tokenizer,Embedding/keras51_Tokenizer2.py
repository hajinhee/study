from tensorflow.keras.preprocessing.text import Tokenizer

text1 = '나는 진짜 매우 jonna 맛있는 밥을 진짜 마구 마구 먹었다.'
text2 = '나는 매우 매우 잘생긴 지구용사 태권브이'

token = Tokenizer()
token.fit_on_texts([text1,text2])  
#print(token.word_index)                         # 두 문장 합쳐서 중복없이 인덱스번호 매겨서 11개로 분류. 가장 빈도수가 높은 어절이 앞으로 옴.

x = token.texts_to_sequences([text1,text2])    
#print(x)                                       # [[2, 3, 1, 5, 6, 7, 3, 4, 4, 8], [2, 1, 1, 9, 10, 11]]
#리스트는 자유분방해서 그 안에 값이 여러형태로 들어가서 shape형태를 제공하지 않는다.
#print(len(x))  2라고 나온다. 그 안에 2개의 큰 덩어리가 있다 그정도.

x = x[0] + x[1]         # (9,) + (6,)
# print(x)              [2, 3, 1, 5, 6, 7, 3, 4, 4, 8, 2, 1, 1, 9, 10, 11]
# print(len(x))         16

from tensorflow.keras.utils import to_categorical
word_size = len(token.word_index)                
#print(word_size)                               # 단어 label값이 11개이므로 11개.

x = to_categorical(x)
print(x) 
'''
[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
'''
print(x.shape)          # (16, 12)          어절이 16개라서 그대로 16. 앞에 인덱스번호0이 추가되서 12.

#원핫인코딩 방식의 문제점. 라벨값이 길어질수록 수없이 많은 0이 들어가고 데이터가 용량이 점점커진다.
#분류모델에서야 라벨값이 많아봐야 얼마나 많겠냐만은 text를 계속이어붙이고 수많은 어절,문장,책을 원핫인코딩하면 문제가 심각해진다.
#일차원에서의 원핫인코딩 방식에서 2차원으로에서 각 단어가 가지는 속성 특징별로 위치 시키고 새로 들어오는 단어를 상관관계를 생각해 좌표에 위치시킨다.
#좌표를 계속 생성해준다 2차원으로.  2차원 공간에 단어들을 계속 특성,성질 등을 구분하여 좌표를 계속 생성해준다.
