
f = lambda x: x**2 - 4*x + 6

def f2(x):
    return x**2 - 4*x + 6

gradient = lambda x: 2*x -4  # gradient는 f에 대한 기울기

def gradient2(x):
    return 2*x-4

# 초기세팅값
x = -10
epochs = 25
learning_rate = 0.25

print("step\t x\t\t f(x)")
print("{:02d}\t {:6.5f}\t {:6.5f}\t".format(0, x, f(x)))      # 초기값 출력

for i in range(epochs):
    x = x - learning_rate * gradient(x)
    print("{:02d}\t  {:6.5f}\t {:1.5f}\t".format(i+1, x, f(x)))

'''
step     x               f(x)
00       -10.00000       146.00000
01        -4.00000       38.00000
02        -1.00000       11.00000
03        0.50000        4.25000
04        1.25000        2.56250
05        1.62500        2.14062
06        1.81250        2.03516
07        1.90625        2.00879
08        1.95312        2.00220
09        1.97656        2.00055
10        1.98828        2.00014
11        1.99414        2.00003
12        1.99707        2.00001
13        1.99854        2.00000
14        1.99927        2.00000
15        1.99963        2.00000
16        1.99982        2.00000
17        1.99991        2.00000
18        1.99995        2.00000
19        1.99998        2.00000
20        1.99999        2.00000
21        1.99999        2.00000
22        2.00000        2.00000
23        2.00000        2.00000
24        2.00000        2.00000
25        2.00000        2.00000
'''